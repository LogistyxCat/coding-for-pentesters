#!/usr/bin/python

#Name: Riley Scott
#Date: 2017-11-03
#Version: 1.0
#Description: Web checking robot

import robotparser

sites = ['www.google.com','www.offensive-security.com','www.yahoo.com']

def getDenies(site):
    paths = []

    #create a new robot parser instance and read the site's robots file
    robot = robotparser.RobotFileParser()
    robot.set_url("http://"+site+"/robots.txt")
    robot.read()

    #for each entry, look at the rule lines and add the path to paths if diallowed
    for entry in robot.entries:
        for line in entry.rulelines:
            not line.allowance and paths.append(line.path)
    return set(paths)

for site in sites:
    print "Denies for " + site
    print "\t" + "\n\t".join(getDenies(site))
